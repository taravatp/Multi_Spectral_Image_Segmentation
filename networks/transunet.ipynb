{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taravatp/Multi_Spectral_Image_Segmentation/blob/main/networks/transunet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "8VEnYD26S8lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Xkxp594zZiqk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cd /content/drive/MyDrive/Vision_Impulse_Task"
      ],
      "metadata": {
        "id": "mCc2OovRzEya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecae7d9a-634a-429c-a9e3-ed739b42425b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Vision_Impulse_Task\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ml_collections"
      ],
      "metadata": {
        "id": "RRwtFPepzH0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629e1253-dcbb-4689-a93e-5e9ffb112590"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ml_collections\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml_collections) (6.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml_collections) (21.6.0)\n",
            "Building wheels for collected packages: ml_collections\n",
            "  Building wheel for ml_collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml_collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94506 sha256=289170c0ed290b8d1340eb3e453906125bb34ae215ac88a43933b595df8377ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n",
            "Successfully built ml_collections\n",
            "Installing collected packages: ml_collections\n",
            "Successfully installed ml_collections-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "h7FKw0mqSsk9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.modules.utils import _pair\n",
        "import math\n",
        "import copy\n",
        "import numpy as np\n",
        "import vit_seg_configs as configs\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Paths to weights of pretrained VIT"
      ],
      "metadata": {
        "id": "7_bvTksl_-OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
        "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
        "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
        "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
        "FC_0 = \"MlpBlock_3/Dense_0\"\n",
        "FC_1 = \"MlpBlock_3/Dense_1\"\n",
        "ATTENTION_NORM = \"LayerNorm_0\"\n",
        "MLP_NORM = \"LayerNorm_2\""
      ],
      "metadata": {
        "id": "rHQCMJJA_7M2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation functions"
      ],
      "metadata": {
        "id": "PH6oWqVyS3-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "\n",
        "def np2th(weights, conv=False):\n",
        "    if conv:\n",
        "        weights = weights.transpose([3, 2, 0, 1])\n",
        "    return torch.from_numpy(weights)"
      ],
      "metadata": {
        "id": "8EQ5jIKPS4Ug"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNet Encoder"
      ],
      "metadata": {
        "id": "0dISO62eTeEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convolution_module(in_channels,out_channels):\n",
        "  conv = nn.Sequential(\n",
        "      nn.Conv2d(in_channels,out_channels,kernel_size=3, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(out_channels,out_channels,kernel_size=3, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(inplace=True)\n",
        "  )\n",
        "  return conv"
      ],
      "metadata": {
        "id": "gJM7St95TFAN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels=12, feat_channels=[32, 64, 128, 256]):\n",
        "\n",
        "    super(CNN_encoder, self).__init__()\n",
        "\n",
        "    # Encoder convolutions\n",
        "    self.down_conv1 = convolution_module(in_channels,32)\n",
        "    self.down_conv2 = convolution_module(32,64)\n",
        "    self.down_conv3 = convolution_module(64,128)\n",
        "    self.down_conv4 = convolution_module(128,256)\n",
        "    self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.out_channels = 256\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    x1 = self.down_conv1(x)\n",
        "    x_low1 = self.max_pool(x1)\n",
        "\n",
        "    x2 = self.down_conv2(x_low1)\n",
        "    x_low2 = self.max_pool(x2)\n",
        "\n",
        "    x3 = self.down_conv3(x_low2)\n",
        "    x_low3 = self.max_pool(x3)\n",
        "\n",
        "    x4 = self.down_conv4(x_low3)\n",
        "    # x_low4 = self.max_pool(x4)\n",
        "\n",
        "    # x5 = self.down_conv5(x_low4)\n",
        "\n",
        "    return x4,[x3,x2,x1]"
      ],
      "metadata": {
        "id": "jRU2pp0rTswj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Modules"
      ],
      "metadata": {
        "id": "NmP-tj2YUzBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, config, in_channels):\n",
        "        super(Embeddings, self).__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.img_size = 128\n",
        "        self.feature_size = 8\n",
        "        self.patch_size = 1\n",
        "        self.patch_size_real = 16\n",
        "\n",
        "        img_size = _pair(self.img_size) #(64,64)\n",
        "        feature_size = _pair(self.feature_size) #()\n",
        "        patch_size = _pair(self.patch_size) #(1,1)\n",
        "        patch_size_real = _pair(self.patch_size_real) #(8,8)\n",
        "\n",
        "        self.n_patches = (feature_size[0] // patch_size[0]) * (feature_size[1] // patch_size[1]) # if patching applied on the main image\n",
        "        self.n_patches_real = (img_size[0] // patch_size_real[0]) * (img_size[1] // patch_size_real[1]) # if patching applied on the feature map\n",
        "\n",
        "        self.patch_embeddings = nn.Conv2d(in_channels=in_channels, out_channels=config.hidden_size, kernel_size=patch_size, stride=patch_size)\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, self.n_patches_real, config.hidden_size))\n",
        "        self.dropout = nn.Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embeddings(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(-1, -2)\n",
        "        embeddings = x + self.position_embeddings[:,:self.n_patches,:]\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "3bo4h9YYUT2-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
        "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.out = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.attn_dropout = nn.Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "        self.proj_dropout = nn.Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        key_layer = key_layer.transpose(-1, -2)\n",
        "        attention_scores = torch.matmul(query_layer, key_layer) #because the input tensors have 4 dimensions, this function performs batch multiplication\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = self.softmax(attention_scores)\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        attention_output = self.out(context_layer)\n",
        "        attention_output = self.proj_dropout(attention_output)\n",
        "\n",
        "        return attention_output"
      ],
      "metadata": {
        "id": "-xY0umNFeBmp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Mlp, self).__init__()\n",
        "        self.fc1 = nn.Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
        "        self.fc2 = nn.Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
        "        self.act_fn = ACT2FN[\"gelu\"]\n",
        "        self.dropout = nn.Dropout(config.transformer[\"dropout_rate\"])\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
        "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "19TZWbjcsLj7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Block, self).__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.attention_norm = nn.LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn_norm = nn.LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn = Mlp(config)\n",
        "        self.attn = Attention(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        x = self.attention_norm(x) #[8,256,768] => [batchsize, number_of_tokens, hidden_size]\n",
        "        x = self.attn(x) #[8,256,768] => [batchsize, number_of_tokens, hidden_size]\n",
        "        x = x + h\n",
        "\n",
        "        h = x\n",
        "        x = self.ffn_norm(x) #[8,256,768] => [batchsize, number_of_tokens, hidden_size]\n",
        "        x = self.ffn(x) #[8,256,768] => [batchsize, number_of_tokens, hidden_size]\n",
        "        x = x + h\n",
        "        return x\n",
        "\n",
        "    def load_from(self, weights, n_block):\n",
        "\n",
        "      ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
        "\n",
        "      with torch.no_grad():\n",
        "        query_weight = np2th(weights[os.path.join(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "        key_weight = np2th(weights[os.path.join(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "        value_weight = np2th(weights[os.path.join(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "        out_weight = np2th(weights[os.path.join(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "\n",
        "        query_bias = np2th(weights[os.path.join(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
        "        key_bias = np2th(weights[os.path.join(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
        "        value_bias = np2th(weights[os.path.join(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
        "        out_bias = np2th(weights[os.path.join(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
        "\n",
        "        self.attn.query.weight.copy_(query_weight)\n",
        "        self.attn.key.weight.copy_(key_weight)\n",
        "        self.attn.value.weight.copy_(value_weight)\n",
        "        self.attn.out.weight.copy_(out_weight)\n",
        "        self.attn.query.bias.copy_(query_bias)\n",
        "        self.attn.key.bias.copy_(key_bias)\n",
        "        self.attn.value.bias.copy_(value_bias)\n",
        "        self.attn.out.bias.copy_(out_bias)\n",
        "\n",
        "        mlp_weight_0 = np2th(weights[os.path.join(ROOT, FC_0, \"kernel\")]).t()\n",
        "        mlp_weight_1 = np2th(weights[os.path.join(ROOT, FC_1, \"kernel\")]).t()\n",
        "        mlp_bias_0 = np2th(weights[os.path.join(ROOT, FC_0, \"bias\")]).t()\n",
        "        mlp_bias_1 = np2th(weights[os.path.join(ROOT, FC_1, \"bias\")]).t()\n",
        "\n",
        "        self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
        "        self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
        "        self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
        "        self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
        "\n",
        "        attention_norm_weight  = np2th(weights[os.path.join(ROOT, ATTENTION_NORM, \"scale\")])\n",
        "        attention_norm_bias = np2th(weights[os.path.join(ROOT, ATTENTION_NORM, \"bias\")])\n",
        "        ffn_norm_weight = np2th(weights[os.path.join(ROOT, MLP_NORM, \"scale\")])\n",
        "        ffn_norm_bias = np2th(weights[os.path.join(ROOT, MLP_NORM, \"bias\")])\n",
        "\n",
        "        self.attention_norm.weight.copy_(attention_norm_weight)\n",
        "        self.attention_norm.bias.copy_(attention_norm_bias)\n",
        "        self.ffn_norm.weight.copy_(ffn_norm_weight)\n",
        "        self.ffn_norm.bias.copy_(ffn_norm_bias)\n"
      ],
      "metadata": {
        "id": "hg3CYEIEsYqK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layer = nn.ModuleList()\n",
        "        self.encoder_norm = nn.LayerNorm(config.hidden_size, eps=1e-6)\n",
        "\n",
        "        for _ in range(config.transformer[\"num_layers\"]):\n",
        "            layer = Block(config)\n",
        "            self.layer.append(copy.deepcopy(layer))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        for layer_block in self.layer:\n",
        "            hidden_states = layer_block(hidden_states)\n",
        "\n",
        "        encoded = self.encoder_norm(hidden_states)\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "N9j73qPlsnxz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN decoder"
      ],
      "metadata": {
        "id": "XFbu1qLSxkxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_decoder(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super(CNN_decoder,self).__init__()\n",
        "\n",
        "    self.config = config\n",
        "    self.first_conv = nn.Conv2d(config.hidden_size,256, kernel_size=1, stride = 1, padding=0)\n",
        "\n",
        "    self.upsample1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "    self.conv1 = convolution_module(256, 128)\n",
        "\n",
        "    self.upsample2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "    self.conv2 = convolution_module(128, 64)\n",
        "\n",
        "    self.upsample3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
        "    self.conv3 = convolution_module(64, 32)\n",
        "\n",
        "    # self.upsample4 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
        "    # self.conv4 = convolution_module(32, 16)\n",
        "\n",
        "    self.out = nn.Conv2d(32,3,kernel_size=1)\n",
        "\n",
        "  def crop_tensor(self,target_tensor,input_tensor):\n",
        "    target_tensor_size = target_tensor.size()[2]\n",
        "    input_tensor_size = input_tensor.size()[2]\n",
        "    delta = input_tensor_size - target_tensor_size\n",
        "    delta = delta // 2\n",
        "    cropped_tensor = input_tensor[:,:,delta:input_tensor_size-delta,delta:input_tensor_size-delta]\n",
        "    return cropped_tensor\n",
        "\n",
        "  def forward(self,hidden_states, features):\n",
        "\n",
        "    # reshaping the hidden_states\n",
        "    batch_size, n_patch, hidden_size = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
        "    H, W = int(np.sqrt(n_patch)), int(np.sqrt(n_patch))\n",
        "    hidden_states = hidden_states.permute(0, 2, 1) # (batch_size, hidden_size, n_patch)\n",
        "    hidden_states = hidden_states.contiguous().view(batch_size, hidden_size, H, W)\n",
        "\n",
        "    #one convolution layer for changing the number of channels\n",
        "    x = self.first_conv(hidden_states)\n",
        "    [x1,x2,x3] = features\n",
        "\n",
        "    x = self.upsample1(x)\n",
        "    y = self.crop_tensor(x,x1)\n",
        "    x = self.conv1(torch.cat([x,y],1))\n",
        "\n",
        "    x = self.upsample2(x)\n",
        "    y = self.crop_tensor(x,x2)\n",
        "    x = self.conv2(torch.cat([x,y],1))\n",
        "\n",
        "    x = self.upsample3(x)\n",
        "    y = self.crop_tensor(x,x3)\n",
        "    x = self.conv3(torch.cat([x,y],1))\n",
        "\n",
        "    # x = self.upsample4(x)\n",
        "    # y = self.crop_tensor(x,x4)\n",
        "    # x = self.conv4(torch.cat([x,y],1))\n",
        "\n",
        "    x = self.out(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "XEMFrp1BxEKe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building TransUNET"
      ],
      "metadata": {
        "id": "0m1TKW8YyPdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trans_Unet(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super(Trans_Unet, self).__init__()\n",
        "\n",
        "    self.config = config\n",
        "    self.conv_encoder = CNN_encoder()\n",
        "    self.embedding = Embeddings(config, in_channels=self.conv_encoder.out_channels)\n",
        "    self.transformer_encoder = Encoder(config)\n",
        "    self.conv_decoder = CNN_decoder(config)\n",
        "    # self.conv_layer = nn.Conv2d(in_channels=self.conv_encoder.out_channels, out_channels=3, kernel_size=1, stride=1, padding = 0)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x, features = self.conv_encoder(x)\n",
        "    # x = self.conv_layer(x)\n",
        "    embeddings = self.embedding(x)\n",
        "    hidden_states = self.transformer_encoder(embeddings)\n",
        "    output = self.conv_decoder(hidden_states, features)\n",
        "    return output\n",
        "\n",
        "  def load_from(self,weights):\n",
        "\n",
        "    with torch.no_grad():\n",
        "      embedding_kernel = np2th(weights[\"embedding/kernel\"], conv=True)[:,0:self.conv_encoder.out_channels,:,:]\n",
        "      embedding_bias = np2th(weights[\"embedding/bias\"])\n",
        "\n",
        "      self.embedding.patch_embeddings.weight.copy_(embedding_kernel)\n",
        "      self.embedding.patch_embeddings.bias.copy_(embedding_bias)\n",
        "\n",
        "      self.transformer_encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "      self.transformer_encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\n",
        "      posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
        "      posemb_new = self.embedding.position_embeddings\n",
        "\n",
        "      # Encoder whole\n",
        "      for bname, block in self.transformer_encoder.named_children():\n",
        "          for uname, unit in block.named_children():\n",
        "              unit.load_from(weights, n_block=uname)"
      ],
      "metadata": {
        "id": "I7yJBDc2yRJC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  CONFIGS = {\n",
        "    'ViT-B_16': configs.get_b16_config(),\n",
        "    'ViT-B_32': configs.get_b32_config(),\n",
        "    'ViT-L_16': configs.get_l16_config(),\n",
        "    'ViT-L_32': configs.get_l32_config(),\n",
        "    'ViT-H_14': configs.get_h14_config(),\n",
        "    'R50-ViT-B_16': configs.get_r50_b16_config(),\n",
        "    'R50-ViT-L_16': configs.get_r50_l16_config(),\n",
        "    'testing': configs.get_testing(),\n",
        "  }\n",
        "\n",
        "  config = CONFIGS['ViT-B_16']\n",
        "  image = torch.rand((8,12,64,64))\n",
        "  model = Trans_Unet(config)\n",
        "\n",
        "  output = model(image)\n",
        "  print('output:',output.shape)\n",
        "\n",
        "  path_wights = '/content/drive/MyDrive/Vision_Impulse_Task/R50+ViT-B_16.npz'\n",
        "  weights = np.load(path_wights)\n",
        "  model.load_from(weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9tvD9zKyUcX",
        "outputId": "63c258df-0d35-42d2-d92c-cb0ed8d090b7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output: torch.Size([8, 3, 64, 64])\n"
          ]
        }
      ]
    }
  ]
}